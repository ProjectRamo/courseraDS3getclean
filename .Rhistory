predict
round(predict)
test
predict[1]
test[1]
test[,1]
test[1,]
test[1,5:]
test[1,5:9]
test[1,11:15]
test[1,18:21]
test[1,19:21]
test[1,18:25]
test[1,18:24]
predict[1]
test[1,18:24]-[predict[1]]
test[1,18:24]-predict[1]
test[1,18:24]-predict[[1]
]
test[1,18:24]-predict[[1]]
round(test[1,18:24]-predict[[1]])
abs(round(test[1,18:24]-predict[[1]]))
abs(round(test[,18:24]-predict[[]]))
abs(round(test[,18:24]-predict[]))
abs(round(test[,18:24]-predict))
predict
predict[]
predict[[]]
predict[]
predict[][]
abs(round(test[1,18:24]-predict[[1]]))
sum(abs(round(test[1,18:24]-predict[[1]])))
sum(abs(round(test[,18:24]-predict[[,]])))
sum(abs(round(test[,18:24]-predict[[:]])))
sum(abs(round(test[,18:24]-predict[[]])))
predict[[:]]
predict[[,]]
predict[
]
sum(abs(round(test[,18:24]-predict[])))
sum(abs(round(test[1:5,18:24]-predict[1:5])))
predict[1]
predict[2]
predict[1:2]
predict[[1:2]]
predict[][1:2]
predict[1:2][]
predict[1:2]
predict[1,1]
predict[1]
predict[1][1]
predict[1][2]
predict[1][1]
predict[2][1]
predict[:][1]
predict[,][1]
predict[][1]
predict[1]
p<-predict[1]
p
p[1]
[[p]]
p[[]]
predict[[]]
predict[
]
predict[1,]
predict[,1]
predict[,]
predict[1]
predict[[[1]]]
predict[[1]]
predict[[2]]
predict[1:5]
sum(abs(round(test[1:5,18:24]-predict[1:5])))
sum(abs(round(test[1,18:24]-predict[1])))
sum(abs(round(test[1,18:24]-predict[[1]])))
sum(abs(round(test[1:2,18:24]-predict[[1:2]])))
sum(abs(round(test[1:2,18:24]-predict[[1:]])))
sum(abs(round(test[1:2,18:24]-predict[[-1]])))
sum(abs(round(test[1:,18:24]-predict[[1:]])))
sum(abs(round(test[1:-1,18:24]-predict[[1:-1]])))
sum(abs(round(test[0:-1,18:24]-predict[[0:-1]])))
predict[[0]]
predict[[1]]
sum(abs(round(test[1:-1,18:24]-predict[[1:-1]])))
sum(abs(round(test[1:length(test),18:24]-predict[[1:-1]])))
length(test)
length(test[,])
length(test[1,])
length(test[,1])
sum(abs(round(test[1:length(test[,1]),18:24]-predict[[1:length(predict)]])))
length(predict)
predict[1:48695]
predict[[1:48695]]
sum(abs(round(test[1:length(test[,1]),18:24]-predict[1:length(predict)])))
test[1:length(test[,1]),18:24]
predict[[1:48695]]
predict[1:48695]
sum(abs(round(test[1:length(test[,1]),18:24]-predict[1:length(predict)])))
sum(abs(round(test[1:100,18:24]-predict[1:100])))
sum(abs(round(test[1:2,18:24]-predict[1:2])))
test[1:2,18:24]-predict[1:2]
test[1:2,18:24]-predict[[1:2]]
sum(abs(round(test[1:length(test[,1]),18:24]-predict[[1:length(predict)]])))
sum(abs(round(test[1:length(test[,1]),18:24]-predict[[1:length(test[,1])]])))
sum(abs(round(test[1:length(test[,1]),18:24]-predict[[1:48695]])))
unlist(predict)
unlist(predict[1])
unlist(predict[])
unlist(predict[1])
unlist(predict[2)
unlist(predict[1:5])
predict[1:5])
predict[1:5])
predict[1:5]
predict[[1:5]
]
predict[[1]:[5]]
predict[1:5]
predict[1:5][]
predict[1:5][[]]
predict[1:5][
]
predict[][1:5]
predict[1:5]
predict[1]
predict[[1]]
predict[[1:2]]
predict[[1][]]
predict[[1][1]]
predict[[1]]
predict[1:2]
test[1:2]-predict[1:2]
test[,1:2]-predict[1:2]
test[1:2,18:24]-predict[1:2]
test[1:2,18:24]
test[1:2,19:24]
test[1:2,18:24]
test[1:2,18:24][]
test[1:2,18:24]
test[1,18:24]-predict[[1]]
1:length(test)
1:length(test[1,])
1:length(test[,1])
indexes=sample(2,nrow(check_data),replace=TRUE, prob=c(.5,.5))
train <- check_data[indexes==1,]
test <- check_data[indexes==2,]
this_tree <- ctree (this_model, train)
this_model<-aggmodel
this_model<-allmodel
this_tree <- ctree (this_model, train)
library("party")
this_tree <- ctree (this_model, train)
predict <- predict (this_tree, test)
predictdf <- data.frame(t(sapply(predict,c)))
sum(abs(round(test[1,18:24]-predict[[1]]))==0)
sum(abs(round(test[,18:24]-predictdf))==0)
sum(abs(round(test[,18:24]-test[,18:24])))==0)
sum(abs(round(test[,18:24]-test[,18:24]))==0)
hitrate = sum(abs(round(test[,18:24]-predictdf))==0)
perfectscore = sum(abs(round(test[,18:24]-test[,18:24]))==0)
hitrate/perfectscore
howgoodagg <- function(check_data, this_model) {
indexes=sample(2,nrow(check_data),replace=TRUE, prob=c(.5,.5))
train <- check_data[indexes==1,]
test <- check_data[indexes==2,]
this_tree <- ctree (this_model, train)
predict <- predict (this_tree, test)
predictdf <- data.frame(t(sapply(predict,c)))
hitrate = sum(abs(round(test[,18:24]-predictdf))==0)
perfectscore = sum(abs(round(test[,18:24]-test[,18:24]))==0)
return (hitrate/perfectscore)
}
howgoodagg(choice_data,allmodel)
train_data$penultimate <- ave(train_data$shopping_pt, train_data$customer_ID, FUN=max)-1
policy_penultimate <- train_data[train_data$shopping_pt==train_data$penultimate,c("A","B","C","D","E","F","G")]
solution_penultimate <- cbind(customers, policy_penultimate)
colnames policy_penultimate
colnames(policy_penultimate)
solution_penultimate
pu_pt <- sqldf("select customer_ID, Max(shopping_pt)-1 as PU_PT from train_data group by customer_ID")
library("sqldf")
pu_pt <- sqldf("select customer_ID, Max(shopping_pt)-1 as PU_PT from train_data group by customer_ID")
pu_train <- sqldf("select train_data.* from pu_pt INNER JOIN train_data on train_data.shopping_pt = pu_pt.PU_PT AND train_data.customer_ID = pu_pt.CUSTOMER_ID")
first_pt <- sqldf("select customer_ID, min(shopping_pt) as FIRST_PT from train_data group by customer_ID")
first_train <- sqldf("select train_data.* from first_pt INNER JOIN train_data on train_data.shopping_pt = first_pt.FIRST_PT AND train_data.customer_ID = first_pt.CUSTOMER_ID")
pu_data <- cbind(choice_data, pu_train['A'], pu_train['B'], pu_train['C'], pu_train['D'], pu_train['E'], pu_train['F'], pu_train['G'])
colnames (pu_data) [26:32] <- c("pua", "pub", "puc", "pud", "pue", "puf", "pug")
pu_data
colnames(pu_data)
pu_all_model <- allmodel <- A + B + C + D + E + F + G ~ married_couple + age_youngest + age_oldest + car_value + car_age + C_previous + homeowner + state + group_size + location + risk_factor + duration_previous + pua + pub + puc + pud + pue + puf + pug
howgoodagg(pu_data,pu_all_model)
pu_all_model
allmodel
howgoodagg(pu_data,pu_all_model)
colnames(pu_data)
pu_all_model
allmodel <- A + B + C + D + E + F + G ~ married_couple + age_youngest + age_oldest + car_value + car_age + C_previous + homeowner + state + group_size + location + risk_factor + duration_previous
howgoodagg(choice_data,allmodel)
howgoodagg(pu_data,pu_all_model)
this_tree
this_model
this_model
allmodel
pu_all_model
this_model
check_data<-pu_data
this_model <- pu_all_model
indexes=sample(2,nrow(check_data),replace=TRUE, prob=c(.5,.5))
train <- check_data[indexes==1,]
test <- check_data[indexes==2,]
this_tree <- ctree (this_model, train)
pu_all_model <- A + B + C + D + E + F + G ~ married_couple + age_youngest + age_oldest + car_value + car_age + C_previous + homeowner + state + group_size + location + risk_factor + duration_previous + pua + pub + puc + pud + pue + puf + pug
howgoodagg(pu_data,pu_all_model)
pu_all_model <- A + B + C + D + E + F + G ~ married_couple + age_youngest + age_oldest + car_value + car_age + C_previous + homeowner + state + group_size + location + risk_factor + duration_previous + pua + pub + puc + pud + pue + puf + pug
pu_model <- A + B + C + D + E + F + G ~ pua + pub + puc + pud + pue + puf + pug
howgoodagg(pu_data,pu_model)
choice_data
colnames(choice_data)
average(choice_data$shopping_pt)
mean(choice_data$shopping_pt)
median(choice_data$shopping_pt)
max(choice_data$shopping_pt)
min(choice_data$shopping_pt)
first_pt <- sqldf("select customer_ID, min(shopping_pt) as FIRST_PT from train_data group by customer_ID")
first_train <- sqldf("select train_data.* from first_pt INNER JOIN train_data on train_data.shopping_pt = first_pt.FIRST_PT AND train_data.customer_ID = first_pt.CUSTOMER_ID")
diff_data <- cbind(choice_data,first_train['A'] - pu_train['A'], first_train['B'] - pu_train['B'], first_train['C'] - pu_train['C'], first_train['D'] - pu_train['D'], first_train['E'] - pu_train['E'], first_train['F']-pu_train['F'], first_train['G'] - pu_train['G'])
colnames (diff_data) [26:32] <- c("diffa", "diffb", "diffc", "diffd", "diffe", "difff", "diffg")
table(diffa)
table(diff_data$diffa)
table(diff_data$diffb)
hist(diff_data$diffa)
plot(table(diff_data$diffb))
table(diff_data$diffa)
hist(plot(table(diff_data$diffb)))
hist(plot(table(diff_data$diffb)))
table(diff_data$diffa)
table(diff_data$diffb)
table(diff_data$diffc)
table(diff_data$diffd)
table(diff_data$diffe)
table(diff_data$difff)
table(diff_data$diffg)
table(diff_data$diffe)
table(diff_data$difff)
table(diff_data$difff)
ticks<-c("RAX","SBUX","HLF")
yahooStockAll <- function (ticker) {
yhootick=paste0('http://ichart.finance.yahoo.com/table.csv?s=',ticker,'&ignore=.csv')
stock=read.csv(yhootick)
stock_rel<-stock[c(1,6,7)]
names(stock_rel)[c(2,3)]<-c(paste0(ticker,".Vol"),paste0(ticker,".adjclose"))
return (stock_rel)
}
yahooStockPeriod <- function (ticker, fromdate,tilldate) {
fromday=as.POSIXlt(fromdate)$mday
frommon=as.POSIXlt(fromdate)$mon # January = 0, so add 1 to mon
fromyear=as.POSIXlt(fromdate)$year+1900
tillday=as.POSIXlt(tilldate)$mday
tillmon=as.POSIXlt(tilldate)$mon # January = 0, so add 1 to mon
tillyear=as.POSIXlt(tilldate)$year+1900
yhootick=paste0('http://ichart.finance.yahoo.com/table.csv?s=',ticker,'&a=',frommon,'&b=',fromday,'&c=',fromyear,'&d=',tillmon,'&e=',tillday,'&f=',tillyear,'&g=d&ignore=.csv')
stock=read.csv(yhootick)
stock_rel<-stock[c(1,6,7)]
names(stock_rel)[c(2,3)]<-c(paste0(ticker,".Vol"),paste0(ticker,".adjclose"))
return (stock_rel)
}
pastdays <-function(daysby=365, todate=Sys.Date()) {
fromdate=todate-daysby
return(fromdate)
}
to=Sys.Date()-1
from=pastdays(30,to)
SPY=yahooStockPeriod("SPY",from,to)
stocks=SPY$Date
for (tick in ticks){
stock=yahooStockPeriod(tick,from,to)
stocks=cbind(stocks,stock)
}
load("C:/Users/ohaneef/Copy/mydocs/Project Code/Rcode/MarketIndex/SP500statsmemoryplus.RData")
symbols <- read.csv("C:/Users/ohaneef/Downloads/sp500.csv", header = F, stringsAsFactors = F)
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
# Oil vs Houston Realty
source ("Quandl_Zillow_library.R")
.libPaths("H:/R/win-library/3.2")
library(lubridate)
require (quantmod)
require (tseries)
require (timeDate)
require (PerformanceAnalytics)
sdate="2015-01-01"
nrg_list = read.csv('Nasdaqenergycompanylist.csv') # This was a download of energy companies only
library(Quandl)
library(lubridate)
Quandl.auth('vxMtozHVJ3L2q7pjM8sP')
#require (quantmod)
#require (tseries)
#require (timeDate)
require (PerformanceAnalytics)
require(quantstrat)
require(TTR)
# Step 1: Get the data
# load Natural Gas front months
sdate="2012-01-01"
edate=today()
NG1.df=Quandl(paste("CHRIS/CME_NN",as.character(1),sep=""), start_date=sdate,end_date=edate)[,c('Date','Settle')]
colnames(NG1.df)[2]="NG1"
NG1 = as.xts(read.zoo(NG1.df))
NG_dailyret=diff(NG1)/lag(NG1, k=-1)
NGlog_dailyret=diff(log(NG1))
plot(NG1)
momentum_signal = function(prices, window=3, tolerance=1) {
#window is the number of data points back we look for momentum
#tolerance is the number of standard deviations out we look for a buy/sell signal
NG1=prices
# Step 2: Create your indicator
n=window
ngroc = na.omit(ROC(NG1, n=n, "discrete")) #dropna so make sure this input works
colnames(ngroc)=paste("lag",as.character(n),sep="")
#r <- t(apply(-ngroc, 1, rank)) # If you want to rank them to do a long/short strategy
ngsd = sd(ngroc)
ngmean = mean(ngroc)
# Step 3: Construct your trading rule
sig <- lag(ifelse(abs(ngroc) < ngmean+tolerance*ngsd, 0, ifelse(ngroc>0,1,-1)))
return(sig)
}
sig = momentum_signal(NG1, 360, 0)
# Step 4: The trading rules/equity curve
ret <- ROC(NG1)*sig
#ret <- ret[â2009-06-02/2010-09-07â²]
eq <- exp(cumsum(na.omit(ret)))
plot(eq)
# Step 5: Evaluate strategy performance
table.Drawdowns(ret, top=10)
table.DownsideRisk(ret)
charts.PerformanceSummary(ret)
library(Quandl)
.libPaths("H:/R/win-library/3.2")
library(lubridate)
Quandl.auth('vxMtozHVJ3L2q7pjM8sP')
#require (quantmod)
#require (tseries)
#require (timeDate)
require (PerformanceAnalytics)
require(quantstrat)
require(TTR)
# Step 1: Get the data
# load Natural Gas front months
sdate="2012-01-01"
edate=today()
NG1.df=Quandl(paste("CHRIS/CME_NN",as.character(1),sep=""), start_date=sdate,end_date=edate)[,c('Date','Settle')]
colnames(NG1.df)[2]="NG1"
NG1 = as.xts(read.zoo(NG1.df))
NG_dailyret=diff(NG1)/lag(NG1, k=-1)
NGlog_dailyret=diff(log(NG1))
plot(NG1)
momentum_signal = function(prices, window=3, tolerance=1) {
#window is the number of data points back we look for momentum
#tolerance is the number of standard deviations out we look for a buy/sell signal
NG1=prices
# Step 2: Create your indicator
n=window
ngroc = na.omit(ROC(NG1, n=n, "discrete")) #dropna so make sure this input works
colnames(ngroc)=paste("lag",as.character(n),sep="")
#r <- t(apply(-ngroc, 1, rank)) # If you want to rank them to do a long/short strategy
ngsd = sd(ngroc)
ngmean = mean(ngroc)
# Step 3: Construct your trading rule
sig <- lag(ifelse(abs(ngroc) < ngmean+tolerance*ngsd, 0, ifelse(ngroc>0,1,-1)))
return(sig)
}
sig = momentum_signal(NG1, 15, 0)
# Step 4: The trading rules/equity curve
ret <- ROC(NG1)*sig
#ret <- ret[â2009-06-02/2010-09-07â²]
eq <- exp(cumsum(na.omit(ret)))
plot(eq)
# Step 5: Evaluate strategy performance
table.Drawdowns(ret, top=10)
table.DownsideRisk(ret)
charts.PerformanceSummary(ret)
sig = momentum_signal(NG1)
ret <- ROC(NG1)*sig
#ret <- ret[â2009-06-02/2010-09-07â²]
eq <- exp(cumsum(na.omit(ret)))
plot(eq)
# Step 5: Evaluate strategy performance
table.Drawdowns(ret, top=10)
table.DownsideRisk(ret)
charts.PerformanceSummary(ret)
write.csv(output, file="run_analysis_output.txt", row.name=FALSE)
output <- run_analysis()
# This script is supposed to perform the following:
# 1. Merges the training and the test sets to create one data set.
# 2. Extracts only the measurements on the mean and standard deviation for each measurement.
# 3. Uses descriptive activity names to name the activities in the data set
# 4. Appropriately labels the data set with descriptive variable names.
# 5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject
# However, changing the order makes it easier, so we will actually do
library(dplyr)
library(tidyr)
run_analysis <- function() {
# I prefer to have the data set in a different directory but the course calls for same directory.
# This function helps me toggle quickly
addpath <- function(filename) {
path = "UCI HAR Dataset/"
path="" # remove for my home laptop
return(paste0(path, filename))
}
# Note that sep="" which means that all consecutive white spaces are joined together. This made a big difference.
# Initially I was getting many extra features (columns) and observations (rows) because each double space generated
# a new feature.
# We know the number of observations should equal the rows of y_train and features equal the rows of features.txt
activity_labels <- read.csv(addpath("activity_labels.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
features <- read.csv(addpath("features.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
x_train <- read.csv(addpath("train/X_train.txt"), header = FALSE, sep="")
y_train <-read.csv(addpath("train/y_train.txt"), header = FALSE, sep="")
subject_train <- read.csv(addpath("train/subject_train.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
x_test <- read.csv(addpath("test/X_test.txt"), header = FALSE, sep="")
y_test <- read.csv(addpath("test/y_test.txt"), header = FALSE, sep="")
subject_test <- read.csv(addpath("test/subject_test.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
# give them the appropriate labels
featurelist <- features$V2
colnames(x_test) = featurelist
colnames(x_train) = featurelist
# fortunately we can merge the labels pretty easily because y_test and activity_labels index on "v1"
# lets merge on activity and store the activity label in a new column called "activity"
x_test$activity = (merge(y_test, activity_labels) %>% mutate(activity=V2))$activity # %>% select(activity) won't work
x_train$activity = (merge(y_train, activity_labels) %>% mutate(activity=V2))$activity
# add the subject id too because we will eventually need it
# the subject stays anonymous so we never merge the label on
x_train$subject = subject_train$V1
x_test$subject = subject_test$V1
# merge the test and train data now that activity has been added
x_test_train <- rbind.data.frame(x_train, x_test)
# Let's extract all the columns which feature the word "std" or "mean(" we need the \\ so that ( doesn't read as a special char
valid_colnames <- make.names(names=names(x_test_train), unique=TRUE, allow_ = TRUE)
names(x_test_train) <- valid_colnames # without this, I get duplicate names because the "-" doesnt distinguish the names properly
x_test_train = select(x_test_train, matches("std|mean\\.|activity|subject"))
# Finally make the new dataframe by gathering the varialbe names, summarizing it and displaying
x_test_train %>%
gather (sensor, reading, 1:73) %>%
group_by(activity, subject, sensor) %>%
summarize(avg_reading=mean(reading)) %>%
# spread (variable, avg) %>% I like it tall and skinny (normalized) but if you want, you could spread() it back out
return
}
output <- run_analysis()
write.csv(output, file="run_analysis_output.txt", row.name=FALSE)
setwd("C:/Users/ohaneef/Dropbox/Coursera/DS3DataCleaning/week4/UCI HAR Dataset")
# This script is supposed to perform the following:
# 1. Merges the training and the test sets to create one data set.
# 2. Extracts only the measurements on the mean and standard deviation for each measurement.
# 3. Uses descriptive activity names to name the activities in the data set
# 4. Appropriately labels the data set with descriptive variable names.
# 5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject
# However, changing the order makes it easier, so we will actually do
library(dplyr)
library(tidyr)
run_analysis <- function() {
# I prefer to have the data set in a different directory but the course calls for same directory.
# This function helps me toggle quickly
addpath <- function(filename) {
path = "UCI HAR Dataset/"
path="" # remove for my home laptop
return(paste0(path, filename))
}
# Note that sep="" which means that all consecutive white spaces are joined together. This made a big difference.
# Initially I was getting many extra features (columns) and observations (rows) because each double space generated
# a new feature.
# We know the number of observations should equal the rows of y_train and features equal the rows of features.txt
activity_labels <- read.csv(addpath("activity_labels.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
features <- read.csv(addpath("features.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
x_train <- read.csv(addpath("train/X_train.txt"), header = FALSE, sep="")
y_train <-read.csv(addpath("train/y_train.txt"), header = FALSE, sep="")
subject_train <- read.csv(addpath("train/subject_train.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
x_test <- read.csv(addpath("test/X_test.txt"), header = FALSE, sep="")
y_test <- read.csv(addpath("test/y_test.txt"), header = FALSE, sep="")
subject_test <- read.csv(addpath("test/subject_test.txt"), header = FALSE, stringsAsFactors=FALSE, sep="")
# give them the appropriate labels
featurelist <- features$V2
colnames(x_test) = featurelist
colnames(x_train) = featurelist
# fortunately we can merge the labels pretty easily because y_test and activity_labels index on "v1"
# lets merge on activity and store the activity label in a new column called "activity"
x_test$activity = (merge(y_test, activity_labels) %>% mutate(activity=V2))$activity # %>% select(activity) won't work
x_train$activity = (merge(y_train, activity_labels) %>% mutate(activity=V2))$activity
# add the subject id too because we will eventually need it
# the subject stays anonymous so we never merge the label on
x_train$subject = subject_train$V1
x_test$subject = subject_test$V1
# merge the test and train data now that activity has been added
x_test_train <- rbind.data.frame(x_train, x_test)
# Let's extract all the columns which feature the word "std" or "mean(" we need the \\ so that ( doesn't read as a special char
valid_colnames <- make.names(names=names(x_test_train), unique=TRUE, allow_ = TRUE)
names(x_test_train) <- valid_colnames # without this, I get duplicate names because the "-" doesnt distinguish the names properly
x_test_train = select(x_test_train, matches("std|mean\\.|activity|subject"))
# Finally make the new dataframe by gathering the varialbe names, summarizing it and displaying
x_test_train %>%
gather (sensor, reading, 1:73) %>%
group_by(activity, subject, sensor) %>%
summarize(avg_reading=mean(reading)) %>%
# spread (variable, avg) %>% I like it tall and skinny (normalized) but if you want, you could spread() it back out
return
}
output <- run_analysis()
write.csv(output, file="run_analysis_output.txt", row.name=FALSE)
write.table(output, file="run_analysis_output.txt", row.name=FALSE)
